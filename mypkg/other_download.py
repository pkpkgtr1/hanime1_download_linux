import sqlite3
import os
import time
from tqdm import tqdm
import requests
import mypkg
import re
import datetime
from mypkg.playwright_html import playwright_html
from lxml import html
import json
from opencc import OpenCC
from mypkg.requests_html import requests_html

def traditional_to_simplified(text: str) -> str:
    cc = OpenCC('t2s')
    """å°†ç¹ä½“ä¸­æ–‡è½¬æ¢ä¸ºç®€ä½“ä¸­æ–‡"""
    return cc.convert(text)

def db_insert_xzzt(LF_ID,table_name,resolution):
    table_name = table_name.replace(' ', '_')
    try:
        # è¿æ¥æ•°æ®åº“
        conn = sqlite3.connect(f"./db/{table_name}.db")
        # åˆ›å»ºæ¸¸æ ‡å¯¹è±¡
        cursor = conn.cursor()
        # æ‰§è¡Œæ’å…¥æ“ä½œ
        cursor.execute(f"UPDATE '{table_name}' SET sfxz='1' ,resolution='{resolution}' WHERE id='{LF_ID}'")
        # æäº¤æ›´æ”¹
        conn.commit()
    except sqlite3.Error as e:
        mypkg.logger.error(f"âŒï¸ é”™è¯¯ï¼š{e}")
    finally:
        # ç¡®ä¿å…³é—­è¿æ¥
        if 'conn' in locals():
            conn.close()
'''
def download_file(resolution,lf_id, NY, url, filename=None):
    """
    ä¸‹è½½æ–‡ä»¶å¹¶æ˜¾ç¤ºè¿›åº¦æ¡
    :param url: æ–‡ä»¶çš„URLåœ°å€
    :param filename: ä¿å­˜çš„æ–‡ä»¶åï¼Œè‹¥Noneåˆ™ä½¿ç”¨URLä¸­çš„æ–‡ä»¶å
    """
    try:
        # å‘é€HTTP GETè¯·æ±‚
        response = requests.get(url, timeout=30, stream=True)
        response.raise_for_status()  # å¦‚æœå“åº”çŠ¶æ€ç ä¸æ˜¯2xxï¼Œä¼šæŠ›å‡ºå¼‚å¸¸

        # è·å–æ–‡ä»¶æ€»å¤§å°
        file_size = int(response.headers.get('Content-Length', 0))

        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ–‡ä»¶åï¼Œä»URLä¸­æå–
        if filename is None:
            filename = url.split('/')[-1]

        # åˆ›å»ºtqdmè¿›åº¦æ¡
        progress_bar = tqdm(total=file_size, unit='B', unit_scale=True, desc=filename)

        # å†™å…¥æ–‡ä»¶
        with open(filename, 'wb') as file:
            for chunk in response.iter_content(chunk_size=1024):
                if chunk:  # filter out keep-alive chunks
                    file.write(chunk)
                    progress_bar.update(len(chunk))

        progress_bar.close()
        mypkg.logger.info(f"ğŸ–¼ï¸ ä¸‹è½½å®Œæˆï¼Œæ–‡ä»¶å·²ä¿å­˜ä¸ºï¼š{filename}")
        #db_insert_xzzt(lf_id, str(NY),resolution)
        return True
    except requests.exceptions.RequestException as e:
        mypkg.logger.error(f"âœ…ï¸ ä¸‹è½½å¤±è´¥ï¼š{e}")
        return False
    except Exception as e:
        mypkg.logger.error(f"âŒï¸ å‘ç”Ÿé”™è¯¯ï¼š{e}")
        return False
    except KeyboardInterrupt:
        mypkg.logger.error(f"âŒï¸ ä¸‹è½½å·²å–æ¶ˆ")
        return False
'''

def download_file(url, filepath, chunk_size=1024*1024):
    """
    æ”¯æŒæ–­ç‚¹ç»­ä¼ çš„ä¸‹è½½å‡½æ•°ï¼ˆä¿®å¤å·²ä¸‹è½½å®Œæˆæ—¶å†æ¬¡æ‰§è¡ŒæŠ¥é”™çš„é—®é¢˜ï¼‰

    :param url: ä¸‹è½½é“¾æ¥
    :param filepath: æœ¬åœ°ä¿å­˜è·¯å¾„
    :param chunk_size: æ¯æ¬¡å†™å…¥çš„å—å¤§å° (é»˜è®¤ 1MB)
    """
    try:
        # è·å–è¿œç¨‹æ–‡ä»¶å¤§å°
        head = requests.head(url, allow_redirects=True)
        head.raise_for_status()
        total_size = int(head.headers.get('Content-Length', 0))

        # å·²ä¸‹è½½å¤§å°
        local_size = os.path.getsize(filepath) if os.path.exists(filepath) else 0

        # å¦‚æœå·²ä¸‹è½½å®Œæˆï¼Œç›´æ¥è¿”å›
        if local_size >= total_size and total_size > 0:
            mypkg.logger.info(f"âœ… æ–‡ä»¶å·²å®Œæ•´å­˜åœ¨ï¼Œæ— éœ€é‡æ–°ä¸‹è½½: {filepath}")
            return True

        # è®¾ç½® Range è¯·æ±‚å¤´
        headers = {}
        if local_size > 0:
            headers['Range'] = f'bytes={local_size}-'

        # å‘é€è¯·æ±‚
        resp = requests.get(url, headers=headers, stream=True)
        resp.raise_for_status()

        # æ‰“å¼€æ¨¡å¼
        mode = 'ab' if local_size > 0 else 'wb'
        mypkg.logger.info(f"å¼€å§‹ä¸‹è½½: {url}")
        mypkg.logger.info(f"ç›®æ ‡æ–‡ä»¶: {filepath}")
        mypkg.logger.info(f"æ€»å¤§å°: {total_size/1024/1024:.2f} MB (å·²ä¸‹è½½ {local_size/1024/1024:.2f} MB)")

        # å†™æ–‡ä»¶
        with open(filepath, mode) as f:
            downloaded = local_size
            for chunk in resp.iter_content(chunk_size=chunk_size):
                if chunk:
                    f.write(chunk)
                    downloaded += len(chunk)
                    percent = downloaded * 100 / total_size
                    print(f"\rè¿›åº¦: {percent:.2f}% ({downloaded/1024/1024:.2f} MB)", end="")
        print()
        mypkg.logger.info(f"{filepath},ä¸‹è½½å®Œæˆï¼")
        return True
    except Exception as e:
        mypkg.logger.error(f"âŒ {filepath},ä¸‹è½½å¤±è´¥{e}")
        mypkg.logger.debug(f"ğŸ {filepath},ä¸‹è½½å¤±è´¥ï¼Œé”™è¯¯ä»£ç ï¼š{e}")
        return False


def db_hanime_init(db, id_list):
    # åˆ›å»ºæ•°æ®åº“è¿æ¥
    db=db.replace(' ', '_')
    conn = sqlite3.connect(f'./db/{db}.db')
    cursor = conn.cursor()
    # åˆ›å»ºè¡¨
    cursor.execute('''CREATE TABLE IF NOT EXISTS '{}'
                    (ID INT PRIMARY KEY NOT NULL, -- é‡Œç•ªID
                    name_jp TEXT COMMENT 'æ—¥æ–‡åç§°',
                    name_cn TEXT COMMENT 'ä¸­æ–‡åç§°',
                    company TEXT COMMENT 'åˆ¶ä½œå…¬å¸',
                    release_date TEXT COMMENT 'å‘è¡Œæ—¥æœŸ',
                    content TEXT COMMENT 'å†…å®¹',
                    img_url TEXT COMMENT 'å›¾ç‰‡URL',    
                    resolution TEXT COMMENT 'åˆ†è¾¨ç‡',             
                    tags TEXT COMMENT 'æ ‡ç­¾',
                    sfxz TEXT COMMENT 'æ˜¯å¦ä¸‹è½½',
                    bj_img_url TEXT COMMENT 'èƒŒæ™¯å›¾url',
                    heji TEXT COMMENT 'åˆé›†'
                    )'''.format(str(db)))
    # å‚æ•°åŒ–æŸ¥è¯¢ï¼Œé˜²æ­¢SQLæ³¨å…¥
    conn.commit()
    placeholders = ",".join("?" * len(id_list))
    try:
        query = f"SELECT id FROM '{db}' WHERE id IN ({placeholders})"
        cursor.execute(query, id_list)
        db_ids = [row[0] for row in cursor.fetchall()]
        missing_ids = list(set(id_list) - set(db_ids))
        conn.close()
        return missing_ids
    except Exception as e:
        mypkg.logger.error("âŒï¸ æŸ¥è¯¢"+f'./db/{db}.db'+'å¤±è´¥')

def db_inster_tag(db, lf_id, name_jp, name_cn, company, content,sfxz, tags,bjimg_url,LF_HEJI,resolution):
    # åˆ›å»ºæ•°æ®åº“è¿æ¥
    db = db.replace(' ', '_')
    conn = sqlite3.connect(f'./db/{db}.db')
    cursor = conn.cursor()
    # åˆ›å»ºè¡¨
    cursor.execute('''CREATE TABLE IF NOT EXISTS '{}'
                    (ID INT PRIMARY KEY NOT NULL, -- é‡Œç•ªID
                    name_jp TEXT COMMENT 'æ—¥æ–‡åç§°',
                    name_cn TEXT COMMENT 'ä¸­æ–‡åç§°',
                    company TEXT COMMENT 'åˆ¶ä½œå…¬å¸',
                    release_date TEXT COMMENT 'å‘è¡Œæ—¥æœŸ',
                    content TEXT COMMENT 'å†…å®¹',
                    img_url TEXT COMMENT 'å›¾ç‰‡URL',    
                    resolution TEXT COMMENT 'åˆ†è¾¨ç‡',             
                    tags TEXT COMMENT 'æ ‡ç­¾',
                    sfxz TEXT COMMENT 'æ˜¯å¦ä¸‹è½½',
                    bj_img_url TEXT COMMENT 'èƒŒæ™¯å›¾url',
                    heji TEXT COMMENT 'åˆé›†'
                    )'''.format(str(db)))
        # å‚æ•°åŒ–æŸ¥è¯¢ï¼Œé˜²æ­¢SQLæ³¨å…¥
    conn.commit()
    try:
        cursor.execute(
            """INSERT OR REPLACE INTO '{}' 
               (id, name_jp, name_cn, company, content, tags, bj_img_url, sfxz, heji, resolution)
               VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)""".format(str(db)),
            (lf_id, name_jp, name_cn, company, content, tags, bjimg_url, sfxz, LF_HEJI,resolution)
        )



        conn.commit()
        mypkg.logger.info(f"âœ…ï¸ é‡Œç•ªidï¼š{lf_id} æ›´æ–°{cursor.rowcount}æ¡æˆåŠŸ")
    except sqlite3.IntegrityError:
            mypkg.logger.error(f"âŒï¸ æ’å…¥idï¼š{lf_id} å¤±è´¥")
            mypkg.logger.debug(f"âŒï¸ æ’å…¥idï¼š{lf_id} å¤±è´¥,åŸå› ï¼š{sqlite3.IntegrityError}")
            # å¦‚æœæ’å…¥å¤±è´¥ï¼Œè¯´æ˜IDå·²ç»å­˜åœ¨ï¼Œå¯ä»¥é€‰æ‹©æ›´æ–°æˆ–è·³è¿‡
    conn.close()

def get_hanime1_page_html(CX,page):
    fetcher = requests_html()
    mypkg.logger.info(f"â³ æ­£åœ¨æŸ¥è¯¢https://hanime1.me/search?genre={CX}&page={page}")
    html = fetcher.get_html(f"https://hanime1.me/search?genre={CX}&page={page}")
    if html:
        if len(html) < 400:
            mypkg.logger.error("âŒï¸ "+html)
        else:
            mypkg.logger.debug("ğŸ è·å–çš„htmlæºç ä¸ºï¼š" + html)
            mypkg.logger.info("ğŸ”„ å¼€å§‹è§£æ" + str(CX) + "htmlæºæ–‡ä»¶")
            return html

def safe_filename_for_linux(name):
    char_map = {
        '!': 'ï¼',  # å…¨è§’æ„Ÿå¹å· (FF01)
        '?': 'ï¼Ÿ',  # å…¨è§’é—®å· (FF1F)
        '<': 'ï¼œ',  # å…¨è§’å°äºå· (FF1C)
        '>': 'ï¼',  # å…¨è§’å¤§äºå· (FF1E)
        ':': 'ï¼š',  # å…¨è§’å†’å· (FF1A)
        '"': 'ï¼‚',  # å…¨è§’åŒå¼•å· (FF02)
        '|': 'ï½œ',  # å…¨è§’ç«–çº¿ (FF5C)
        '\\': 'ï¼¼', # å…¨è§’åæ–œçº¿ (FF3C)
        '/': 'ï¼',  # å…¨è§’æ–œçº¿ (FF0F)
        '*': 'ï¼Š',  # å…¨è§’æ˜Ÿå· (FF0A)
        # ' ': '_',
    }

    for half, full in char_map.items():
        name = name.replace(half, full)

    safe_name = ''
    for char in name:
        if 0 <= ord(char) <= 31:
            safe_name += '_'
        else:
            safe_name += char

    return safe_name

def download_jpg(url, file_name, save_path):
    """
    ä¸‹è½½å•å¼ å›¾ç‰‡å¹¶ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„ï¼Œå…è®¸è‡ªå®šä¹‰æ–‡ä»¶åã€‚
    ä¸‹è½½æˆåŠŸè¿”å›Trueï¼Œå¤±è´¥è¿”å›Falseã€‚
    æ”¯æŒå¤±è´¥é‡è¯•ï¼Œæœ€å¤š5æ¬¡ï¼Œæ¯æ¬¡å¤±è´¥é—´éš”5ç§’ã€‚
    """

    max_retries = 5
    for attempt in range(1, max_retries + 1):
        try:
            # æ£€æŸ¥å¹¶åˆ›å»ºä¿å­˜è·¯å¾„
            # å‘é€HTTP GETè¯·æ±‚è·å–å›¾ç‰‡æ•°æ®
            response = requests.get(url, stream=True, timeout=30)
            response.raise_for_status()  # æ£€æŸ¥HTTPè¯·æ±‚æ˜¯å¦æˆåŠŸ

            # å¦‚æœæœªæä¾›è‡ªå®šä¹‰æ–‡ä»¶åï¼Œæå–URLä¸­çš„æ–‡ä»¶å
            if not file_name:
                file_name = url.split("/")[-1]

            # ç¡®ä¿æ–‡ä»¶è·¯å¾„çš„å®Œæ•´æ€§
            save_file = os.path.join(save_path, file_name)

            # ä»¥äºŒè¿›åˆ¶å†™å…¥æ¨¡å¼ä¿å­˜å›¾ç‰‡
            with open(save_file, 'wb') as file:
                for chunk in response.iter_content(chunk_size=1024):
                    if chunk:
                        file.write(chunk)

            mypkg.logger.info(f"âœ… [ç¬¬{attempt}/{max_retries}æ¬¡å°è¯•]æˆåŠŸä¸‹è½½å›¾ç‰‡å¹¶ä¿å­˜ä¸ºï¼š{save_file} ")
            return True

        except requests.exceptions.RequestException as e:
            mypkg.logger.error(f"âŒ [ç¬¬{attempt}/{max_retries}æ¬¡å°è¯•]ä¸‹è½½å›¾ç‰‡å¤±è´¥ï¼š{e} ")
        except Exception as e:
            mypkg.logger.error(f"âŒ [ç¬¬{attempt}/{max_retries}æ¬¡å°è¯•]ä¿å­˜å›¾ç‰‡å¤±è´¥ï¼š{e} ")
            return False

        time.sleep(8)

def get_hanime1_download(LFID):
    fetcher = requests_html()
    mypkg.logger.info(f"â³ æ­£åœ¨è·å–https://hanime1.me/download?v={LFID}")
    html = fetcher.get_html(f"https://hanime1.me/download?v={LFID}")

    if html:
        if len(html) < 400:
            mypkg.logger.error("âŒï¸ "+html)
        else:
            mypkg.logger.debug("ğŸ è·å–çš„htmlæºç ä¸ºï¼š" + html)
            mypkg.logger.info("ğŸ”„ å¼€å§‹è§£æä¸‹è½½é¡µhtmlæºæ–‡ä»¶")
            return html

def download_move_info(page):

    tree = html.fromstring(page)
    LF_NAME_XP = tree.xpath('//*[@id="content-div"]/div[1]/div[4]/div/div/h3/text()')
    LF_ZL= tree.xpath("//table[@class='download-table']/tbody/tr[contains(@style, 'text-align: center;')]/td[2]/text()")
    #LF_DOWNLOAD_URL= tree.xpath('//*[@id="content-div"]/div[1]/div[4]/div/div/table/tbody/tr[2]/td[5]/a')
    LF_DOWNLOAD_URL= tree.xpath('//a[contains(@class, "exoclick-popunder") and contains(@class, "juicyads-popunder")]')
    rq_info=[]

    data_urls = []
    for a_tag in LF_DOWNLOAD_URL:
        data_url = a_tag.get('data-url')
        if data_url:
            data_urls.append(data_url)
    return LF_NAME_XP,data_urls


def filter_text(arr):
    # åŒ¹é…ä¸­æ–‡ã€æ—¥æ–‡ã€è‹±æ–‡
    pattern = re.compile(r"[\u4e00-\u9fff\u3040-\u30ff\u31f0-\u31ff\uFF66-\uFF9F\u3400-\u4dbfA-Za-z]")
    return [s for s in arr if pattern.search(s)]
def cj_html_ys_download(db, lf_id, html_content,save_file,idx,idy):

        tree = html.fromstring(html_content)
        # ä½¿ç”¨XPathæŸ¥è¯¢åŒ¹é…æ‰€æœ‰å…·æœ‰IDå±æ€§çš„divå…ƒç´ 
        # æ—¥æ–‡å
        jp_name = tree.xpath('//*[@id="shareBtn-title"]/text()')[0]
        jp_name =jp_name.replace("ï”", " ")
        # ä¸­æ–‡å
        #cn_name = tree.xpath('//*[@id="player-div-wrapper"]/div/div/div[2]/text()')
        #cn_name = [s for s in cn_name if re.search(r'[\u4e00-\u9fff]', s)][0]
        # åˆ¶ä½œå…¬å¸
        #LF_ZZGS = tree.xpath('//*[@id="player-div-wrapper"]/div/div/div[3]/text()')
        #LF_ZZGS = [s for s in LF_ZZGS if re.search(r'[\u4e00-\u9fff]', s)][0].replace("\n", "<br>\n")
        # æ ‡ç­¾
        tags = tree.xpath('//*[@id="player-div-wrapper"]/div/div/a/text()')
        tags = filter_text(tags)
        hanime_genre = '\n    '.join(f'<genre>{traditional_to_simplified(x)}</genre>' for x in tags).replace('\xa0', '')
        hanime_tags = '\n    '.join(f'<tag>{traditional_to_simplified(x)}</tag>' for x in tags).replace('\xa0', '')
        tags_cleaned = ','.join([
            traditional_to_simplified(''.join(t.replace('\xa0', '').split()))
            for t in tags if t.strip()
        ])        # å†…å®¹
        LF_NR = tree.xpath('//*[@id="player-div-wrapper"]/div/div/div[3]/text()')#[0].replace("\n", "").replace(" ", "")
        LF_NR =filter_text(LF_NR)
        plot_text = LF_NR[0].replace("\n", "<br>\n")        #é‡Œç•ªæ—¥æœŸ
        LF_RQ= tree.xpath('//*[@id="player-div-wrapper"]/div[6]/div/div/text()')[0].replace("\n", "").replace(" ", "")
        if len(LF_RQ) < 7 :
            LF_RQ = tree.xpath('//*[@id="player-div-wrapper"]/div[7]/div/div/text()')[0].replace("\n", "").replace(" ",                                                                                                                   "")
        LF_RQ=LF_RQ.replace('\xa0', '-')
        print(LF_RQ)
        LFGKS,LF_RQ = LF_RQ.split('--')
        # æ’­æ”¾ç¼©ç•¥å›¾
        LF_SLT =  json.loads(tree.xpath('//script[@type="application/ld+json"]/text()')[0].replace("\n", ""))['thumbnailUrl'][0]
        # åˆé›†
        LF_HEJI = tree.xpath('//*[@id="video-playlist-wrapper"]/div/h4[1]/text()')[0]
        show_nfo = f'''<?xml version="1.0" encoding="utf-8" standalone="yes"?>
            <movie>
             <plot><![CDATA[{plot_text}]]></plot>
            <customrating>{db}</customrating>
            <mpaa>{db}</mpaa>
            <lockdata>false</lockdata>
            <title>{jp_name}</title> 
            <title_jp>{jp_name}</title_jp> 
            <rating></rating> 
            <criticrating></criticrating> 
            <uncensored>True</uncensored> 
            <year>{str(LF_RQ[:4])}</year>
            <premiered>{LF_RQ}</premiered>
            <releasedate>{LF_RQ}</releasedate>
            {hanime_tags}
            <studio>{LF_HEJI}</studio>
            {hanime_genre}
            <set>
            <name>{LF_HEJI}</name>
            </set>
            <art>
            <poster>{jp_name}-poster.png</poster>
            </art>
            <maker>{LF_HEJI}</maker>
            <label>{lf_id}</label>
            <num>{lf_id}</num>
            <release>{LF_RQ}</release>
            <website>https://hanime1.me/watch?v={lf_id}</website>
            </movie>
            '''
        mypkg.logger.info(f"ğŸ¬ï¸ [{idx}/{idy}]ç•ªå‰§è¯¦ç»†ä¿¡æ¯ \nğŸ“ºï¸ æ ‡é¢˜ï¼š{jp_name}\nğŸ†” IDï¼š{lf_id}\nğŸ”— URLï¼šhttps://hanime1.me/watch?v={lf_id}\nğŸ–¼ï¸ ç¼©ç•¥å›¾URLï¼š{LF_SLT}\nâœï¸ ä½œè€…ï¼š{LF_HEJI}\nğŸ”— ä¸‹è½½é“¾æ¥:\nğŸ“… æ—¥æœŸï¼š{LF_RQ}\nğŸ‘ï¸ {LFGKS}\nğŸ“‹ï¸ æè¿°ï¼š{LF_NR}\nğŸ”– æ ‡ç­¾ï¼š{tags_cleaned}")
        gl_jp_name= safe_filename_for_linux(jp_name)
        mypkg.logger.info(f"ğŸ”„ è½¬æ¢å­—æ®µ {jp_name} -> {gl_jp_name}")
        bclj=db.replace(' ', '_')
        if download_jpg(LF_SLT, bclj + '/' + str(lf_id) + "-poster.png", save_path=save_file) == False:
            return False
        if download_jpg(LF_SLT, bclj + '/' + str(lf_id) + "-fanart.jpg", save_path=save_file) == False:
            return False
        try:
            if not os.path.exists(f'{save_file}{bclj}'):
                os.makedirs(f'{save_file}{bclj}')
            download_html = get_hanime1_download(lf_id)
            download_info = download_move_info(download_html)
            if 'æ–°ç•ªé å‘Š' in download_info[0][0]:
                mypkg.logger.info(f"ğŸ“ {download_info[0][0]}æ­¤ç‰‡ä¸ºæ–°ç•ªé å‘Šè·³è¿‡ä¸‹è½½")
            else:
                mypkg.logger.info(f"ğŸ“ å¼€å§‹ä¸‹è½½ï¼š{download_info[1][0]}")
                pattern = r'-(\d{3,4}p)\.mp4'

                match = re.search(pattern, download_info[1][0])
                if match:
                    resolution = match.group(1)
                    mypkg.logger.info(f"ğŸ¯ å½“å‰ä¸‹è½½åˆ†è¾¨ç‡ä¸ºï¼š{resolution}")
                    # print(f"\033[33m{num}.é€‰æ‹©ä¸‹è½½è§†é¢‘è´¨é‡ï¼š {quality}\033[0m")
                else:
                    pass

                download_name = f"{safe_filename_for_linux(download_info[0][0])}"
                if download_file(download_info[1][0],f'{save_file}{bclj}/{str(lf_id)}.mp4') ==False:
                    return False
                with open(save_file+bclj+'/'+ str(lf_id) + '.nfo', 'w', encoding="utf-8") as file:
                    file.write(show_nfo)
                time.sleep(1)
                db_inster_tag(db, lf_id, gl_jp_name, None, LF_HEJI, plot_text, '1', tags_cleaned, LF_SLT, LF_HEJI,
                              resolution)
        except Exception as e:
            mypkg.logger.error(f"âŒ ä¿å­˜å¤±è´¥{e}")
            mypkg.logger.debug(f"ğŸ ä¿å­˜å¤±è´¥ï¼Œé”™è¯¯ä»£ç ï¼š{e}")



def hanime1_id_info(lf_id):
    fetcher = requests_html()
    mypkg.logger.info(f"â³ æ­£åœ¨è·å–https://hanime1.me/watch?v={lf_id}")
    html = fetcher.get_html(f"https://hanime1.me/watch?v={lf_id}")
    if html:
        if len(html) < 400:
            mypkg.logger.error("âŒï¸ "+html)
        else:
            mypkg.logger.debug("ğŸ è·å–çš„htmlæºç ä¸ºï¼š" + html)
            mypkg.logger.info("ğŸ”„ å¼€å§‹è§£æé‡Œç•ªid:" + str(lf_id) + " htmlæºæ–‡ä»¶")
            return html
def gl_id(db, html_content):
    tree = html.fromstring(html_content)
    url_list = tree.xpath('//*[@id="home-rows-wrapper"]/div/div/div/a/@href')
    url_list = list(set(url_list))

    id_list = []
    for url in  url_list:
        if 'https://hanime1.me/watch?v=' in url :

            match = re.search(r'v=(\d+)', url)
            if match:
                id_list.append(int(match.group(1)))

    id_list=db_hanime_init(db,id_list)

    del  html_content
    return id_list





def qtfl_plxz(db,save_file,page=1):
    html_content = get_hanime1_page_html(db,page)         #è·å–html
    id_list=gl_id(db,html_content)
    if id_list == []:
        mypkg.logger.info(f'ğŸ“ {db} åˆ†ç±»,æœªå‘ç°æ›´æ–°')
    else:
        for idx, lf_id in enumerate(id_list, start=1):
            html_content = hanime1_id_info(lf_id)
            try:
                if cj_html_ys_download(db, lf_id, html_content,save_file,idx,len(id_list)) ==False:
                    continue
                time.sleep(1)
            except Exception as e:
                mypkg.logger.error(f"âŒï¸ åˆ®å‰Š{db},id:{lf_id}å¤±è´¥ï¼ŒåŸå› ï¼š" + e)


#å•ä¸ªé‡Œç•ªidä¸‹è½½
def dg_id_download(db,lf_id,idx,id_list,save_file):
    html_content = hanime1_id_info(lf_id)
    try:
        cj_html_ys_download(db, lf_id, html_content, save_file, idx, len(id_list))
    except Exception as e:
        mypkg.logger.error(f"âŒï¸ åˆ®å‰Š{db},id:{lf_id}å¤±è´¥ï¼ŒåŸå› ï¼š" + e)

