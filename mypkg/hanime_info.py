import sqlite3
import os
import time
from tqdm import tqdm
import requests
import mypkg
import re
import datetime
from mypkg.playwright_html import playwright_html
from lxml import html
import json
from opencc import OpenCC
from mypkg.requests_html import requests_html

def traditional_to_simplified(text: str) -> str:
    cc = OpenCC('t2s')
    """å°†ç¹ä½“ä¸­æ–‡è½¬æ¢ä¸ºç®€ä½“ä¸­æ–‡"""
    return cc.convert(text)

#é‡‡é›†å†…å®¹å†™å…¥æ•°æ®åº“
def db_hanime_info(NY, id, LF_NAME_JP, LF_NAME_CN, LF_ZZGS, LF_FSRQ, LF_NR, LF_IMG, LF_TAG):
    # åˆ›å»ºæ•°æ®åº“è¿æ¥
    conn = sqlite3.connect('./db/hanime1.db')
    cursor = conn.cursor()
    # åˆ›å»ºè¡¨
    cursor.execute('''CREATE TABLE IF NOT EXISTS '{}'
                    (ID INT PRIMARY KEY NOT NULL, -- é‡Œç•ªID
                    name_jp TEXT COMMENT 'æ—¥æ–‡åç§°',
                    name_cn TEXT COMMENT 'ä¸­æ–‡åç§°',
                    company TEXT COMMENT 'åˆ¶ä½œå…¬å¸',
                    release_date TEXT COMMENT 'å‘è¡Œæ—¥æœŸ',
                    content TEXT COMMENT 'å†…å®¹',
                    img_url TEXT COMMENT 'å›¾ç‰‡URL',    
                    resolution TEXT COMMENT 'åˆ†è¾¨ç‡',             
                    tags TEXT COMMENT 'æ ‡ç­¾',
                    sfxz TEXT COMMENT 'æ˜¯å¦ä¸‹è½½',
                    bj_img_url TEXT COMMENT 'èƒŒæ™¯å›¾url',
                    heji TEXT COMMENT 'åˆé›†'
                    )'''.format(str(NY)))
    ycz=[]
    for i in range(len(LF_NAME_JP)):
        lf_id = id[i]
        name_jp = LF_NAME_JP[i]
        name_cn = LF_NAME_CN[i]
        company = LF_ZZGS[i]
        release_date = LF_FSRQ[i]
        content = LF_NR[i]
        img_url = LF_IMG[i]
        tags = ','.join(LF_TAG[i])  # å°†æ ‡ç­¾åˆ—è¡¨è½¬æ¢ä¸ºé€—å·åˆ†éš”å­—ç¬¦ä¸²

        # å‚æ•°åŒ–æŸ¥è¯¢ï¼Œé˜²æ­¢SQLæ³¨å…¥
        try:
            cursor.execute('''INSERT INTO '{}' 
                            (id,name_jp, name_cn, company, release_date, content, img_url, tags)
                            VALUES (?, ?, ?, ?, ?, ?, ?, ?)'''.format(str(NY)),
                           (lf_id, name_jp, name_cn, company, release_date, content, img_url, tags))

        except sqlite3.IntegrityError:
            # å¦‚æœæ’å…¥å¤±è´¥ï¼Œè¯´æ˜IDå·²ç»å­˜åœ¨ï¼Œå¯ä»¥é€‰æ‹©æ›´æ–°æˆ–è·³è¿‡
            ycz.append(lf_id)


    if len(ycz) == 0:
        mypkg.logger.info(f"âœ…ï¸ åˆ®å‰Šä¿¡æ¯å·²æˆåŠŸå…¥åº“")
    else:
        mypkg.logger.info(f"âœ…ï¸ é‡Œç•ªID {ycz}å·²å­˜åœ¨ã€‚")

            # æäº¤äº‹åŠ¡
    conn.commit()

    # å…³é—­è¿æ¥
    conn.close()

#è·å–å½“æœˆé‡Œç•ªé¢„å‘Šé¡µhtml
def get_hanime1_xlifan(NY):
    fetcher = requests_html()
    mypkg.logger.info(f"â³ æ­£åœ¨è·å–https://hanime1.me/previews/{NY}")
    html = fetcher.get_html(f"https://hanime1.me/previews/{NY}")
    if html:
        if len(html) < 400:
            mypkg.logger.error("âŒï¸ "+html)
        else:
            mypkg.logger.debug("ğŸ è·å–çš„htmlæºç ä¸ºï¼š" + html)
            mypkg.logger.info("ğŸ”„ å¼€å§‹è§£æ" + str(NY) + "htmlæºæ–‡ä»¶")
            return html
#è§£æhtmlçš„å…ƒç´ å¹¶å…¥åº“
def html_info_to_db(NY, html_content):
    tree = html.fromstring(html_content)
    # ä½¿ç”¨XPathæŸ¥è¯¢åŒ¹é…æ‰€æœ‰å…·æœ‰IDå±æ€§çš„divå…ƒç´ 
    div_elements = tree.xpath('//div[@id]')

    pure_digit_ids = []
    for div in div_elements:
        element_id = div.get('id')
        if element_id is not None and element_id.isdigit():
            pure_digit_ids.append(element_id)

    # è¾“å‡ºç»“æœ
    mypkg.logger.info(f"âœ…ï¸ å·²æˆåŠŸè·å–é‡Œç•ªIDï¼š{pure_digit_ids}")
    # é‡Œç•ªæ—¥æ–‡å
    LF_NAME_JP = []
    # é‡Œç•ªä¸­æ–‡å
    LF_NAME_CN = []
    # åˆ¶ä½œå…¬å¸
    LF_ZZGS = []
    # é‡Œç•ªå‘è¡Œæ—¥æœŸ
    LF_FSRQ = []
    # é‡Œç•ªå†…å®¹
    LF_NR = []
    # é‡Œç•ªå›¾ç‰‡
    LF_IMG = []
    # é‡Œç•ªæ ‡ç­¾
    LF_TAG = []

    for id in pure_digit_ids:
        # print("ID:", id)
        # ä½¿ç”¨XPathæŸ¥è¯¢åŒ¹é…å…·æœ‰ç‰¹å®šIDçš„divå…ƒç´ 
        LF_NAME_JP_XP = tree.xpath(f'//*[@id="{id}"]/div/div[2]/h3/text()')
        LF_NAME_CN_XP = tree.xpath(f'//*[@id="{id}"]/div/div[2]/div/h4/text()')
        LF_ZZGS_XP = tree.xpath(f'//*[@id="{id}"]/div/div[2]/div/h5[1]/a/text()')
        LF_FSRQ_XP = tree.xpath(f'//*[@id="{id}"]/div/div[2]/div/h5[2]/text()')
        LF_NR_XP = tree.xpath(f'//*[@id="{id}"]/div/div[2]/div/h5[3]/text()')
        LF_IMG_XP = tree.xpath(f'//*[@id="{id}"]/div/div[1]/img')
        LF_TAG_XP = tree.xpath(f'//*[@id="{id}"]/div/div[2]/div/h5[5]/div/a/text()')

        LF_NAME_JP.append(LF_NAME_JP_XP[0])
        LF_NAME_CN.append(LF_NAME_CN_XP[0])
        LF_ZZGS.append(LF_ZZGS_XP[0])
        dt = datetime.datetime.strptime(LF_FSRQ_XP[0].rstrip(), "%Yå¹´%mæœˆ%dæ—¥")
        LF_FSRQ.append(dt.strftime("%Y-%m-%d"))
        LF_NR.append(LF_NR_XP[0])
        LF_IMG.append(LF_IMG_XP[0].get('src'))
        LF_TAG.append(LF_TAG_XP)


    db_hanime_info(NY, pure_digit_ids, LF_NAME_JP, LF_NAME_CN, LF_ZZGS, LF_FSRQ, LF_NR, LF_IMG, LF_TAG)

'''
def download_jpg(url, file_name, save_path):
        """
        ä¸‹è½½å•å¼ å›¾ç‰‡å¹¶ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„ï¼Œå…è®¸è‡ªå®šä¹‰æ–‡ä»¶åã€‚
        ä¸‹è½½æˆåŠŸè¿”å›Trueï¼Œå¤±è´¥è¿”å›Falseã€‚
        """
        try:
            # æ£€æŸ¥å¹¶åˆ›å»ºä¿å­˜è·¯å¾„
            if not os.path.exists(save_path):
                os.makedirs(save_path)

            # å‘é€HTTP GETè¯·æ±‚è·å–å›¾ç‰‡æ•°æ®
            response = requests.get(url, stream=True, timeout=30)
            response.raise_for_status()  # æ£€æŸ¥HTTPè¯·æ±‚æ˜¯å¦æˆåŠŸ

            # å¦‚æœæœªæä¾›è‡ªå®šä¹‰æ–‡ä»¶åï¼Œæå–URLä¸­çš„æ–‡ä»¶å
            if not file_name:
                file_name = url.split("/")[-1]

            # ç¡®ä¿æ–‡ä»¶è·¯å¾„çš„å®Œæ•´æ€§
            save_file = os.path.join(save_path, file_name)

            # ä»¥äºŒè¿›åˆ¶å†™å…¥æ¨¡å¼ä¿å­˜å›¾ç‰‡
            with open(save_file, 'wb') as file:
                for chunk in response.iter_content(chunk_size=1024):
                    if chunk:
                        file.write(chunk)
            mypkg.logger.info(f"âœ…ï¸ æˆåŠŸä¸‹è½½å›¾ç‰‡å¹¶ä¿å­˜ä¸ºï¼š{save_file}")
            return True
        except requests.exceptions.RequestException as e:
            mypkg.logger.error(f"âŒï¸ ä¸‹è½½å›¾ç‰‡å¤±è´¥ï¼š{e}")
            return False
        except Exception as e:
            mypkg.logger.error(f"âŒï¸ ä¿å­˜å›¾ç‰‡å¤±è´¥ï¼š{e}")
            return False
'''

def download_jpg(url, file_name, save_path):
    """
    ä¸‹è½½å•å¼ å›¾ç‰‡å¹¶ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„ï¼Œå…è®¸è‡ªå®šä¹‰æ–‡ä»¶åã€‚
    ä¸‹è½½æˆåŠŸè¿”å›Trueï¼Œå¤±è´¥è¿”å›Falseã€‚
    æ”¯æŒå¤±è´¥é‡è¯•ï¼Œæœ€å¤š5æ¬¡ï¼Œæ¯æ¬¡å¤±è´¥é—´éš”5ç§’ã€‚
    """
    max_retries = 5
    for attempt in range(1, max_retries + 1):
        try:
            # å‘é€HTTP GETè¯·æ±‚è·å–å›¾ç‰‡æ•°æ®
            response = requests.get(url, stream=True, timeout=30)
            response.raise_for_status()  # æ£€æŸ¥HTTPè¯·æ±‚æ˜¯å¦æˆåŠŸ

            # å¦‚æœæœªæä¾›è‡ªå®šä¹‰æ–‡ä»¶åï¼Œæå–URLä¸­çš„æ–‡ä»¶å
            if not file_name:
                file_name = url.split("/")[-1]

            # ç¡®ä¿æ–‡ä»¶è·¯å¾„çš„å®Œæ•´æ€§
            save_file = os.path.join(save_path, file_name)

            # ä»¥äºŒè¿›åˆ¶å†™å…¥æ¨¡å¼ä¿å­˜å›¾ç‰‡
            with open(save_file, 'wb') as file:
                for chunk in response.iter_content(chunk_size=1024):
                    if chunk:
                        file.write(chunk)

            mypkg.logger.info(f"âœ… [ç¬¬{attempt}/{max_retries}æ¬¡å°è¯•]æˆåŠŸä¸‹è½½å›¾ç‰‡å¹¶ä¿å­˜ä¸ºï¼š{save_file} ")
            return True

        except requests.exceptions.RequestException as e:
            mypkg.logger.error(f"âŒ [ç¬¬{attempt}/{max_retries}æ¬¡å°è¯•]ä¸‹è½½å›¾ç‰‡å¤±è´¥ï¼š{e} ")
        except Exception as e:
            mypkg.logger.error(f"âŒ [ç¬¬{attempt}/{max_retries}æ¬¡å°è¯•]ä¿å­˜å›¾ç‰‡å¤±è´¥ï¼š{e} ")
            return False

        time.sleep(8)

    # æ‰€æœ‰é‡è¯•å¤±è´¥

def get_table_data(table_name):
    """
    è·å–æŒ‡å®šæ•°æ®åº“è¡¨ä¸­çš„æ‰€æœ‰æ•°æ®ã€‚

    Args:
        table_name (str): è¡¨åã€‚

    Returns:
        list: åŒ…å«è¡¨ä¸­æ‰€æœ‰è®°å½•çš„åˆ—è¡¨ï¼Œæ¯ä¸ªè®°å½•æ˜¯ä¸€ä¸ªå…ƒç»„ã€‚
    """
    try:
        # è¿æ¥æ•°æ®åº“
        conn = sqlite3.connect("./db/hanime1.db")
        # åˆ›å»ºæ¸¸æ ‡å¯¹è±¡
        cursor = conn.cursor()
        # æ‰§è¡ŒæŸ¥è¯¢ï¼Œè·å–æ‰€æœ‰è®°å½•
        cursor.execute(f"SELECT * FROM '{table_name}' Where sfxz='0'")
        # æå–æ‰€æœ‰è®°å½•
        rows = cursor.fetchall()
        return rows
    except sqlite3.Error as e:
        mypkg.logger.error(f"âŒï¸ é”™è¯¯ï¼š{e}")
        return []
    finally:
        # ç¡®ä¿å…³é—­è¿æ¥
        if 'conn' in locals():
            conn.close()

def get_table_data_null(table_name):
    """
    è·å–æŒ‡å®šæ•°æ®åº“è¡¨ä¸­çš„æ‰€æœ‰æ•°æ®ã€‚

    Args:
        table_name (str): è¡¨åã€‚

    Returns:
        list: åŒ…å«è¡¨ä¸­æ‰€æœ‰è®°å½•çš„åˆ—è¡¨ï¼Œæ¯ä¸ªè®°å½•æ˜¯ä¸€ä¸ªå…ƒç»„ã€‚
    """
    try:
        # è¿æ¥æ•°æ®åº“
        conn = sqlite3.connect("./db/hanime1.db")
        # åˆ›å»ºæ¸¸æ ‡å¯¹è±¡
        cursor = conn.cursor()
        # æ‰§è¡ŒæŸ¥è¯¢ï¼Œè·å–æ‰€æœ‰è®°å½•
        cursor.execute(f"SELECT * FROM '{table_name}' Where sfxz is null")
        # æå–æ‰€æœ‰è®°å½•
        rows = cursor.fetchall()
        return rows
    except sqlite3.Error as e:
        mypkg.logger.error(f"âŒï¸ é”™è¯¯ï¼š{e}")
        return []
    finally:
        # ç¡®ä¿å…³é—­è¿æ¥
        if 'conn' in locals():
            conn.close()

def safe_filename_for_linux(name):
    char_map = {
        '!': 'ï¼',  # å…¨è§’æ„Ÿå¹å· (FF01)
        '?': 'ï¼Ÿ',  # å…¨è§’é—®å· (FF1F)
        '<': 'ï¼œ',  # å…¨è§’å°äºå· (FF1C)
        '>': 'ï¼',  # å…¨è§’å¤§äºå· (FF1E)
        ':': 'ï¼š',  # å…¨è§’å†’å· (FF1A)
        '"': 'ï¼‚',  # å…¨è§’åŒå¼•å· (FF02)
        '|': 'ï½œ',  # å…¨è§’ç«–çº¿ (FF5C)
        '\\': 'ï¼¼', # å…¨è§’åæ–œçº¿ (FF3C)
        '/': 'ï¼',  # å…¨è§’æ–œçº¿ (FF0F)
        '*': 'ï¼Š',  # å…¨è§’æ˜Ÿå· (FF0A)
        # ' ': '_',
    }

    for half, full in char_map.items():
        name = name.replace(half, full)

    safe_name = ''
    for char in name:
        if 0 <= ord(char) <= 31:
            safe_name += '_'
        else:
            safe_name += char

    return safe_name

def extract_from_start_to_æŒ‡å®šå†…å®¹(text, æŒ‡å®šå†…å®¹):
  """
  æå–ä»å­—ç¬¦ä¸²å¼€å¤´åˆ°æŒ‡å®šå†…å®¹ä¹‹é—´çš„å†…å®¹ï¼ˆä¸åŒ…æ‹¬æŒ‡å®šå†…å®¹ï¼‰ã€‚

  Args:
    text: è¦å¤„ç†çš„å­—ç¬¦ä¸²ã€‚
    æŒ‡å®šå†…å®¹: ä½œä¸ºç»“æŸæ ‡è®°çš„å­—ç¬¦ä¸²ã€‚

  Returns:
    ä»å¼€å¤´åˆ°æŒ‡å®šå†…å®¹ä¹‹é—´çš„å†…å®¹ï¼Œå¦‚æœæœªæ‰¾åˆ°æŒ‡å®šå†…å®¹ï¼Œåˆ™è¿”å›æ•´ä¸ªå­—ç¬¦ä¸²ã€‚
  """
  pattern = r"^(.*?)" + re.escape(æŒ‡å®šå†…å®¹)
  match = re.search(pattern, text)
  if match:
    return match.group(1)
  else:
    return text

def extract_before_first_space(text):
  """
  æå–å­—ç¬¦ä¸²ä¸­ç¬¬ä¸€ä¸ªç©ºæ ¼ä¹‹å‰çš„å†…å®¹ã€‚

  Args:
    text: è¦å¤„ç†çš„å­—ç¬¦ä¸²ã€‚

  Returns:
    ç¬¬ä¸€ä¸ªç©ºæ ¼ä¹‹å‰çš„å†…å®¹ï¼Œå¦‚æœå­—ç¬¦ä¸²ä¸­æ²¡æœ‰ç©ºæ ¼ï¼Œåˆ™è¿”å›æ•´ä¸ªå­—ç¬¦ä¸²ã€‚
  """
  match = re.search(r'^(\S*) ', text)
  if match:
    return match.group(1)
  else:
    return text

def videos_nfo_jpg(NY,save_file):

    gltj = [' å¾Œç·¨', ' å‰ç·¨', ' ï¼ƒ', ' ç¬¬']
    table_name = str(NY)
    current_time = datetime.datetime.now()
    dt_str = current_time.strftime("%Y-%m-%d %H:%M:%S")
    data = get_table_data(table_name)
    if data:
        for idx, row in enumerate(data, start=1):  # idx ä» 1 å¼€å§‹è®¡æ•°
            # é‡Œç•ªid
            LF_ID = row[0]
            # é‡Œç•ªæ—¥æ–‡å
            LF_NAME_JP = row[1]
            # é‡Œç•ªä¸­æ–‡å
            LF_NAME_CN = row[2]
            # åˆ¶ä½œå…¬å¸
            LF_ZZGS = row[3]
            # é‡Œç•ªå‘è¡Œæ—¥æœŸ
            LF_FSRQ = row[4]
            # é‡Œç•ªå†…å®¹
            LF_NR = row[5]
            # é‡Œç•ªå›¾ç‰‡
            LF_IMG = row[6]
            # é‡Œç•ªæ ‡ç­¾
            LF_TAG = row[8]
            # èƒŒæ™¯ç¼©ç•¥å›¾
            bj_img_url = row[10]
            # åˆé›†
            LF_HEJI = row[11]

            # æ ‡ç­¾
            tags = LF_TAG.split(',')
            hanime_genre = '\n    '.join(f'<genre>{x}</genre>' for x in tags)
            hanime_tags = '\n    '.join(f'<tag>{x}</tag>' for x in tags)
            img_filename = f"{safe_filename_for_linux(LF_NAME_CN)}".lstrip()

            HJ_NAME_JP = safe_filename_for_linux(LF_NAME_JP).lstrip()
            mypkg.logger.info(f"ğŸ”„ è½¬æ¢å­—æ®µ {LF_NAME_JP} -> {HJ_NAME_JP}")
            if 'OVA ' in HJ_NAME_JP:
                for j in gltj:
                    GL_LF_NAME_JP = extract_from_start_to_æŒ‡å®šå†…å®¹(HJ_NAME_JP, j)
                    if GL_LF_NAME_JP != HJ_NAME_JP:
                        break

            else:
                for j in gltj:
                    GL_LF_NAME_JP = extract_from_start_to_æŒ‡å®šå†…å®¹(HJ_NAME_JP, j)
                    if GL_LF_NAME_JP != HJ_NAME_JP:
                        break
                GL_LF_NAME_JP = extract_before_first_space(GL_LF_NAME_JP)
            plot_text = LF_NR.replace("\n", "<br>\n")
            show_nfo = f'''<?xml version="1.0" encoding="utf-8" standalone="yes"?>
    <movie>
     <plot><![CDATA[{plot_text}]]></plot>
    <customrating>é‡Œç•ª</customrating>
    <mpaa>é‡Œç•ª</mpaa>
    <lockdata>false</lockdata>
    <dateadded>{dt_str}</dateadded>
    <title>{LF_NAME_JP}</title> 
    <title_jp>{LF_NAME_JP}</title_jp> 
    <title_cn>{LF_NAME_CN}</title_cn> 
    <rating></rating> 
    <criticrating></criticrating> 
    <uncensored>True</uncensored> 
    <year>{str(LF_FSRQ[:4])}</year>
    <premiered>{LF_FSRQ.replace('å¹´', '-').replace('æœˆ', '-').replace('æ—¥', '').replace(' ', '')}</premiered>
    <releasedate>{LF_FSRQ.replace('å¹´', '-').replace('æœˆ', '-').replace('æ—¥', '').replace(' ', '')}</releasedate>
    {hanime_tags}
    <studio>{LF_ZZGS}</studio>
    {hanime_genre}
    <set>
    <name>{LF_HEJI}</name>
    </set>
    <art>
    <poster>{LF_NAME_JP}-poster.png</poster>
    </art>
    <maker>{LF_ZZGS}</maker>
    <label>{LF_ID}</label>
    <num>{LF_ID}</num>
    <release>{LF_FSRQ.replace('å¹´', '-').replace('æœˆ', '-').replace('æ—¥', '').replace(' ', '')}</release>
    <website>https://hanime1.me/watch?v={LF_ID}</website>
    </movie>
    '''
            # print(show_nfo)
            nfo_filename = f"{safe_filename_for_linux(LF_NAME_CN)}".lstrip()
            mypkg.logger.info(f"ğŸ”„ è½¬æ¢å­—æ®µ {LF_NAME_CN} -> {nfo_filename}")
            mypkg.logger.info(f"ğŸ”„ posterå›¾ç‰‡ä¸‹è½½åœ°å€ï¼š{LF_IMG}")
            if download_jpg(LF_IMG, img_filename + "-poster.png", save_path=save_file) == False:
                continue
            time.sleep(1)
            mypkg.logger.info(f"ğŸ”„ fanartå›¾ç‰‡ä¸‹è½½åœ°å€ï¼š{bj_img_url}")
            if download_jpg(bj_img_url, img_filename + "-fanart.jpg", save_path=save_file) == False:
                continue
            try:
                download_html =get_hanime1_download(LF_ID)
                download_info = download_move_info(download_html)
                if 'æ–°ç•ªé å‘Š' in download_info[0][0]:
                    mypkg.logger.info(f"ğŸ“ {download_info[0][0]}æ­¤ç‰‡ä¸ºæ–°ç•ªé å‘Šè·³è¿‡ä¸‹è½½")
                else:
                    mypkg.logger.info(f"ğŸ“ å¼€å§‹ä¸‹è½½ï¼š{download_info[0][0]}")
                    pattern = r"-([^.]*)\."


                    match = re.search(pattern, download_info[1][0])
                    if match:
                        resolution = match.group(1)
                        mypkg.logger.info(f"ğŸ¯ å½“å‰ä¸‹è½½åˆ†è¾¨ç‡ä¸ºï¼š{resolution}")
                        # print(f"\033[33m{num}.é€‰æ‹©ä¸‹è½½è§†é¢‘è´¨é‡ï¼š {quality}\033[0m")
                    else:
                        pass
                    download_name=f"{safe_filename_for_linux(download_info[0][0])}"
                    if download_file(str(resolution),LF_ID,NY,download_info[1][0],f'{save_file}{download_name}.mp4') == False:#ä¸‹è½½æ–‡ä»¶
                        continue


                with open(save_file + nfo_filename + '.nfo', 'w', encoding="utf-8") as file:
                    file.write(show_nfo)
                time.sleep(1)
            except Exception as e:
                mypkg.logger.error(f"âŒ ä¿å­˜å¤±è´¥")
                mypkg.logger.debug(f"ğŸ ä¿å­˜å¤±è´¥ï¼Œé”™è¯¯ä»£ç ï¼š{e}")


    else:
        mypkg.logger.info(f"âœ…ï¸ {table_name}å½“æœˆå·²å®Œæˆ")

def db_insert_xzzt(LF_ID,table_name,resolution):
    try:
        # è¿æ¥æ•°æ®åº“
        conn = sqlite3.connect("./db/hanime1.db")
        # åˆ›å»ºæ¸¸æ ‡å¯¹è±¡
        cursor = conn.cursor()
        # æ‰§è¡Œæ’å…¥æ“ä½œ
        cursor.execute(f"UPDATE '{table_name}' SET sfxz='1' ,resolution='{resolution}' WHERE id='{LF_ID}'")
        # æäº¤æ›´æ”¹
        conn.commit()
    except sqlite3.Error as e:
        mypkg.logger.error(f"âŒï¸ é”™è¯¯ï¼š{e}")
    finally:
        # ç¡®ä¿å…³é—­è¿æ¥
        if 'conn' in locals():
            conn.close()

def download_file(resolution,lf_id, NY, url, filename=None):
    """
    ä¸‹è½½æ–‡ä»¶å¹¶æ˜¾ç¤ºè¿›åº¦æ¡
    :param url: æ–‡ä»¶çš„URLåœ°å€
    :param filename: ä¿å­˜çš„æ–‡ä»¶åï¼Œè‹¥Noneåˆ™ä½¿ç”¨URLä¸­çš„æ–‡ä»¶å
    """
    try:
        # å‘é€HTTP GETè¯·æ±‚
        response = requests.get(url, timeout=30, stream=True)
        response.raise_for_status()  # å¦‚æœå“åº”çŠ¶æ€ç ä¸æ˜¯2xxï¼Œä¼šæŠ›å‡ºå¼‚å¸¸

        # è·å–æ–‡ä»¶æ€»å¤§å°
        file_size = int(response.headers.get('Content-Length', 0))

        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ–‡ä»¶åï¼Œä»URLä¸­æå–
        if filename is None:
            filename = url.split('/')[-1]

        # åˆ›å»ºtqdmè¿›åº¦æ¡
        progress_bar = tqdm(total=file_size, unit='B', unit_scale=True, desc=filename)

        # å†™å…¥æ–‡ä»¶
        with open(filename, 'wb') as file:
            for chunk in response.iter_content(chunk_size=1024):
                if chunk:  # filter out keep-alive chunks
                    file.write(chunk)
                    progress_bar.update(len(chunk))

        progress_bar.close()
        mypkg.logger.info(f"ğŸ–¼ï¸ ä¸‹è½½å®Œæˆï¼Œæ–‡ä»¶å·²ä¿å­˜ä¸ºï¼š{filename}")
        db_insert_xzzt(lf_id, str(NY),resolution)
        return True
    except requests.exceptions.RequestException as e:
        mypkg.logger.error(f"âœ…ï¸ ä¸‹è½½å¤±è´¥ï¼š{e}")
        return False
    except Exception as e:
        mypkg.logger.error(f"âŒï¸ å‘ç”Ÿé”™è¯¯ï¼š{e}")
        return False
    except KeyboardInterrupt:
        mypkg.logger.error(f"âŒï¸ ä¸‹è½½å·²å–æ¶ˆ")
        return False

def download_move_info(page):

    tree = html.fromstring(page)
    LF_NAME_XP = tree.xpath('//*[@id="content-div"]/div[1]/div[4]/div/div/h3/text()')
    LF_ZL= tree.xpath("//table[@class='download-table']/tbody/tr[contains(@style, 'text-align: center;')]/td[2]/text()")
    #LF_DOWNLOAD_URL= tree.xpath('//*[@id="content-div"]/div[1]/div[4]/div/div/table/tbody/tr[2]/td[5]/a')
    LF_DOWNLOAD_URL= tree.xpath('//a[contains(@class, "exoclick-popunder") and contains(@class, "juicyads-popunder")]')
    rq_info=[]

    data_urls = []
    for a_tag in LF_DOWNLOAD_URL:
        data_url = a_tag.get('data-url')
        if data_url:
            data_urls.append(data_url)
    return LF_NAME_XP,data_urls

def get_hanime1_download(LFID):
    fetcher = requests_html()
    mypkg.logger.info(f"â³ æ­£åœ¨è·å–https://hanime1.me/download?v={LFID}")
    html = fetcher.get_html(f"https://hanime1.me/download?v={LFID}")

    if html:
        if len(html) < 400:
            mypkg.logger.error("âŒï¸ "+html)
        else:
            mypkg.logger.debug("ğŸ è·å–çš„htmlæºç ä¸ºï¼š" + html)
            mypkg.logger.info("ğŸ”„ å¼€å§‹è§£æä¸‹è½½é¡µhtmlæºæ–‡ä»¶")
            return html

def db_hanime_table():
    conn = sqlite3.connect("./db/hanime1.db")
    cursor = conn.cursor()

    # æŸ¥è¯¢æ‰€æœ‰è¡¨å
    cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
    tables = cursor.fetchall()
    # å…³é—­è¿æ¥
    conn.close()
    return [t[0] for t in tables]

def db_inster_tag(NY, lf_id, name_jp, name_cn, company, content,sfxz, tags,bjimg_url,LF_HEJI):
    # åˆ›å»ºæ•°æ®åº“è¿æ¥
    conn = sqlite3.connect('./db/hanime1.db')
    cursor = conn.cursor()
    # åˆ›å»ºè¡¨
    cursor.execute('''CREATE TABLE IF NOT EXISTS '{}'
                    (ID INT PRIMARY KEY NOT NULL, -- é‡Œç•ªID
                    name_jp TEXT COMMENT 'æ—¥æ–‡åç§°',
                    name_cn TEXT COMMENT 'ä¸­æ–‡åç§°',
                    company TEXT COMMENT 'åˆ¶ä½œå…¬å¸',
                    release_date TEXT COMMENT 'å‘è¡Œæ—¥æœŸ',
                    content TEXT COMMENT 'å†…å®¹',
                    img_url TEXT COMMENT 'å›¾ç‰‡URL',    
                    resolution TEXT COMMENT 'åˆ†è¾¨ç‡',             
                    tags TEXT COMMENT 'æ ‡ç­¾',
                    sfxz TEXT COMMENT 'æ˜¯å¦ä¸‹è½½',
                    bj_img_url TEXT COMMENT 'èƒŒæ™¯å›¾url',
                    heji TEXT COMMENT 'åˆé›†'
                    )'''.format(str(NY)))
        # å‚æ•°åŒ–æŸ¥è¯¢ï¼Œé˜²æ­¢SQLæ³¨å…¥
    try:
        cursor.execute(
            """UPDATE '{}' 
               SET name_jp = ?, 
                   name_cn = ?, 
                   company = ?, 
                   content = ?, 
                   tags = ? ,
                   bj_img_url =? ,
                   sfxz = ? ,
                   heji = ? 
               WHERE id = ?""".format(str(NY)),
            (name_jp, name_cn, company, content, tags, bjimg_url,sfxz,LF_HEJI,lf_id)
        )



        conn.commit()
        conn.close()
        mypkg.logger.info(f"âœ…ï¸ é‡Œç•ªidï¼š{lf_id} æ›´æ–°æˆåŠŸ")
    except sqlite3.IntegrityError:
            mypkg.logger.error(f"âŒï¸ æ’å…¥idï¼š{lf_id} å¤±è´¥")
            mypkg.logger.debug(f"âŒï¸ æ’å…¥idï¼š{lf_id} å¤±è´¥,åŸå› ï¼š{sqlite3.IntegrityError}")
            # å¦‚æœæ’å…¥å¤±è´¥ï¼Œè¯´æ˜IDå·²ç»å­˜åœ¨ï¼Œå¯ä»¥é€‰æ‹©æ›´æ–°æˆ–è·³è¿‡

def hanime1_id_info(lf_id):
    fetcher = requests_html()
    mypkg.logger.info(f"â³ æ­£åœ¨è·å–https://hanime1.me/watch?v={lf_id}")
    html = fetcher.get_html(f"https://hanime1.me/watch?v={lf_id}")
    if html:
        if len(html) < 400:
            mypkg.logger.error("âŒï¸ "+html)
        else:
            mypkg.logger.debug("ğŸ è·å–çš„htmlæºç ä¸ºï¼š" + html)
            mypkg.logger.info("ğŸ”„ å¼€å§‹è§£æé‡Œç•ªid:" + str(lf_id) + " htmlæºæ–‡ä»¶")
            return html

'''
def sx_tags_db(NY):
    data = get_table_data_null(NY)
    if data:
        for idx, row in enumerate(data, start=1):  # idx ä» 1 å¼€å§‹è®¡æ•°
            LF_ID = row[0]
            html =hanime1_id_info(LF_ID)
            tree = html.fromstring(str(html))
            # æ—¥æ–‡å
            jp_name = tree.xpath('//*[@id="shareBtn-title"]/text()')[0]
            # ä¸­æ–‡å
            cn_name = tree.xpath('//*[@id="player-div-wrapper"]/div[6]/div/div[2]/text()')[0]
            # æ ‡ç­¾
            tags = tree.xpath('//*[@id="player-div-wrapper"]/div[7]/div/a/text()')
            tags_cleaned = ', '.join([t.replace('\xa0', '').strip() for t in tags])
            # å†…å®¹
            LF_NR = tree.xpath('//*[@id="player-div-wrapper"]/div[6]/div/div[3]/text()')[0].replace("\n", "<br>\n")
            # åˆ¶ä½œå…¬å¸
            LF_ZZGS = tree.xpath('//*[@id="video-artist-name"]/text()')[0].replace("\n", "").replace(" ", "")
            # æ’­æ”¾ç¼©ç•¥å›¾
            LF_SLT = json.loads(tree.xpath('//script[@type="application/ld+json"]/text()')[0].replace("\n", ""))['thumbnailUrl'][0]
            print(jp_name)
            print(cn_name)
            print(tags_cleaned)
            print(LF_NR)
            print(LF_ZZGS)
            print(LF_SLT)

'''


def sx_tags_db(NY, lf_id,html_content):
    hj_gl = [' THE ANIMATION']
    tree = html.fromstring(html_content)
    # ä½¿ç”¨XPathæŸ¥è¯¢åŒ¹é…æ‰€æœ‰å…·æœ‰IDå±æ€§çš„divå…ƒç´ 
    # æ—¥æ–‡å
    jp_name = tree.xpath('//*[@id="shareBtn-title"]/text()')[0]
    # ä¸­æ–‡å
    cn_name = tree.xpath('//*[@id="player-div-wrapper"]/div/div/div[2]/text()')
    clean_list = [
        s for s in cn_name
        if s.strip() != "" and not re.match(r'^\d{2}:\d{2}$', s)
    ]
    cn_name = [s for s in clean_list if re.search(r'[\u4e00-\u9fff]', s)][0]
    # åˆ¶ä½œå…¬å¸
    LF_ZZGS = tree.xpath('//*[@id="player-div-wrapper"]/div/div/div[3]/text()')
    LF_ZZGS = [s for s in LF_ZZGS if re.search(r'[\u4e00-\u9fff]', s)][0].replace("\n", "<br>\n")
    # æ ‡ç­¾
    tags = tree.xpath('//*[@id="player-div-wrapper"]/div/div/a/text()')
    tags_cleaned = ','.join([
        traditional_to_simplified(''.join(t.replace('\xa0', '').split()))
        for t in tags if t.strip()
    ])
    # å†…å®¹
    LF_NR = tree.xpath('//*[@id="video-artist-name"]/text()')[0].replace("\n", "").replace(" ", "")
    # æ’­æ”¾ç¼©ç•¥å›¾
    #LF_SLT = json.loads(tree.xpath('//script[@type="application/ld+json"]/text()')[0].replace("\n", ""))['thumbnailUrl'][0]
    LF_SLT = tree.xpath("//meta[@property='og:image']/@content")[0]
    # åˆé›†
    LF_HEJI = tree.xpath('//*[@id="video-playlist-wrapper"]/div/h4[1]/text()')[0]
    for j in hj_gl:
        LF_HEJI=re.sub(j, '', LF_HEJI, flags=re.IGNORECASE)
    if '[æ–°ç•ªé å‘Š]' in cn_name:
        db_inster_tag(NY,lf_id,jp_name,cn_name,LF_NR,LF_ZZGS,None,tags_cleaned,LF_SLT,LF_HEJI)
    elif '[ä¸­å­—å¾Œè£œ]' in cn_name:
        db_inster_tag(NY, lf_id, jp_name, cn_name, LF_NR, LF_ZZGS, '0', tags_cleaned, LF_SLT,LF_HEJI)
    else:
        db_inster_tag(NY, lf_id, jp_name, cn_name, LF_NR, LF_ZZGS, '0', tags_cleaned, LF_SLT,LF_HEJI)



def sx_xf_yg_tag(NY):
    data = get_table_data_null(NY)
    if data:
        for idx, row in enumerate(data, start=1):  # idx ä» 1 å¼€å§‹è®¡æ•°
            # é‡Œç•ªid
            LF_ID = row[0]
            try:
                html_content = hanime1_id_info(LF_ID)
                sx_tags_db(NY, LF_ID,html_content)
            except Exception as e:
                mypkg.logger.error(f"âŒï¸ æ›´æ–°é‡Œç•ªid:{LF_ID} tagså¤±è´¥,å¼‚å¸¸åŸå› ï¼š{e}" )


def db_update_url(NY, id, LF_IMG):
    # åˆ›å»ºæ•°æ®åº“è¿æ¥
    conn = sqlite3.connect('./db/hanime1.db')
    cursor = conn.cursor()
    ycz=[]
    for i in range(len(LF_IMG)):
        lf_id = id[i]
        img_url = LF_IMG[i]
        mypkg.logger.info(f"âœ…ï¸ é‡Œç•ªIDï¼š{lf_id},image_urlå·²æ›´æ–°urlä¸ºï¼š{img_url}")
        try:
            cursor.execute('''update '{}'  SET 
                            img_url =?
                           where id= ? '''.format(str(NY)),
                           (img_url,lf_id))

        except sqlite3.IntegrityError:
            # å¦‚æœæ’å…¥å¤±è´¥ï¼Œè¯´æ˜IDå·²ç»å­˜åœ¨ï¼Œå¯ä»¥é€‰æ‹©æ›´æ–°æˆ–è·³è¿‡
            ycz.append(lf_id)

            # æäº¤äº‹åŠ¡
    conn.commit()

    # å…³é—­è¿æ¥
    conn.close()

def update_img_url_to_db(NY, html_content):
    tree = html.fromstring(html_content)
    # ä½¿ç”¨XPathæŸ¥è¯¢åŒ¹é…æ‰€æœ‰å…·æœ‰IDå±æ€§çš„divå…ƒç´ 
    div_elements = tree.xpath('//div[@id]')

    pure_digit_ids = []
    for div in div_elements:
        element_id = div.get('id')
        if element_id is not None and element_id.isdigit():
            pure_digit_ids.append(element_id)

    # è¾“å‡ºç»“æœ
    #mypkg.logger.info(f"âœ…ï¸ å·²æˆåŠŸæ›´æ–°image_url,é‡Œç•ªIDï¼š{pure_digit_ids}")
    # é‡Œç•ªæ—¥æ–‡å

    LF_IMG = []

    for id in pure_digit_ids:
        LF_IMG_XP = tree.xpath(f'//*[@id="{id}"]/div/div[1]/img')
        LF_IMG.append(LF_IMG_XP[0].get('src'))


    db_update_url(NY, pure_digit_ids, LF_IMG)

